{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "data = pd.read_csv('Dataset.csv')\n",
    "data.iloc[np.random.permutation(len(data))]\n",
    "data = data.reset_index(drop=True)\n",
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep].astype(np.float64)\n",
    "data = clean_dataset(data)\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target = data.columns[-1]\n",
    "feature = data.drop(target, axis = 1)\n",
    "label = data.drop(feature, axis = 1)\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(feature, label,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold,cross_validate\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "scoring =['accuracy']\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "def get_avg_eva(clf):\n",
    "    results = cross_validate (estimator=clf,\n",
    "                                X=feature,\n",
    "                                y=label,\n",
    "                                cv=kfold,\n",
    "                                scoring=scoring,)\n",
    "    print('score_time'+str(np.mean(results['score_time'])))\n",
    "    print('accuracy:' + str(np.mean(results['test_accuracy'])))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RBF: \n",
      "score_time8.468435597419738\n",
      "accuracy:0.9107008520277924\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.98      0.94      1736\n",
      "         1.0       0.93      0.69      0.79       647\n",
      "\n",
      "    accuracy                           0.90      2383\n",
      "   macro avg       0.91      0.84      0.87      2383\n",
      "weighted avg       0.90      0.90      0.90      2383\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['clf.joblib']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import joblib\n",
    "clf = svm.SVC()\n",
    "print(\"RBF: \")\n",
    "result_RBF = get_avg_eva(clf)\n",
    "clf.fit(feature_train, label_train)\n",
    "pred = clf.predict(feature_test)\n",
    "print(classification_report(label_test,pred))\n",
    "joblib.dump(clf,'clf.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "log: \n",
      "score_time1.0311553478240967\n",
      "accuracy:0.9529997069745686\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.98      0.97      1736\n",
      "         1.0       0.95      0.87      0.90       647\n",
      "\n",
      "    accuracy                           0.95      2383\n",
      "   macro avg       0.95      0.92      0.94      2383\n",
      "weighted avg       0.95      0.95      0.95      2383\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['clf_log.joblib']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "import math\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "def log_kernel(x,y):\n",
    "    kernel = euclidean_distances(x,y)\n",
    "    kernel = -np.log(kernel+1)\n",
    "    return kernel\n",
    "\n",
    "clf_log = svm.SVC(kernel = log_kernel)\n",
    "print(\"log: \")\n",
    "result_log = get_avg_eva(clf_log) \n",
    "clf_log.fit(feature_train, label_train)\n",
    "pred_log = clf_log.predict(feature_test)\n",
    "print(classification_report(label_test,pred_log))\n",
    "joblib.dump(clf_log,'clf_log.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "linear: \n",
      "score_time0.07166616916656494\n",
      "accuracy:0.8383567471923092\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.89      0.89      1736\n",
      "         1.0       0.71      0.71      0.71       647\n",
      "\n",
      "    accuracy                           0.84      2383\n",
      "   macro avg       0.80      0.80      0.80      2383\n",
      "weighted avg       0.84      0.84      0.84      2383\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "clf_linear = make_pipeline(StandardScaler(),LinearSVC())\n",
    "\n",
    "print(\"linear: \")\n",
    "result_linear = get_avg_eva(clf_linear) \n",
    "clf_linear.fit(feature_train, label_train)\n",
    "pred_linear = clf_linear.predict(feature_test)\n",
    "print(classification_report(label_test,pred_linear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "log: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.98      0.97      1736\n",
      "         1.0       0.95      0.87      0.90       647\n",
      "\n",
      "    accuracy                           0.95      2383\n",
      "   macro avg       0.95      0.92      0.94      2383\n",
      "weighted avg       0.95      0.95      0.95      2383\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import math\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "def log_kernel(x,y):\n",
    "    kernel = euclidean_distances(x,y)\n",
    "    kernel = -np.log(kernel+1)\n",
    "    return kernel\n",
    "\n",
    "clf_log = svm.SVC(kernel = log_kernel)\n",
    "print(\"log: \")\n",
    "clf_log.fit(feature_train, label_train)\n",
    "pred_log = clf_log.predict(feature_test)\n",
    "print(classification_report(label_test,pred_log))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['clf_linear.joblib']"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "joblib.dump(clf_linear,'clf_linear.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}